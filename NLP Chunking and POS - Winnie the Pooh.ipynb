{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74ff79ad",
   "metadata": {},
   "source": [
    "# NLP Chunking and POS - Winnie the Pooh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a0bb37",
   "metadata": {},
   "source": [
    "Novels and text contain insights into ideologies and places that are often originally unknown to the reader. By reading a written piece, you uncover the opinions of the author on their chosen topic and come to understand both the topic and how the author thinks.\n",
    "\n",
    "By the end of this project, you will find out the main topics of discussion in the novel of your choosing and can begin to discern some of the author's thoughts and beliefs!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf3caef",
   "metadata": {},
   "source": [
    "## Import and Preprocess Text Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b207551d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import pos_tag, RegexpParser\n",
    "from nltk.tokenize import PunktSentenceTokenizer, word_tokenize\n",
    "from collections import Counter\n",
    "\n",
    "# import text of winnie the pooh - from project gutenberg\n",
    "text = open(\"winnie.txt\",encoding='utf-8').read().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "7605dbba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentence Tokenizer\n",
    "def word_sentence_tokenize(text):\n",
    "  \n",
    "  # create a PunktSentenceTokenizer\n",
    "  sentence_tokenizer = PunktSentenceTokenizer(text)\n",
    "  \n",
    "  # sentence tokenize text\n",
    "  sentence_tokenized = sentence_tokenizer.tokenize(text)\n",
    "  \n",
    "  # create a list to hold word tokenized sentences\n",
    "  word_tokenized = list()\n",
    "  \n",
    "  # for-loop through each tokenized sentence in sentence_tokenized\n",
    "  for tokenized_sentence in sentence_tokenized:\n",
    "    # word tokenize each sentence and append to word_tokenized\n",
    "    word_tokenized.append(word_tokenize(tokenized_sentence))\n",
    "    \n",
    "  return word_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "da2c4c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that pulls chunks out of chunked sentence and finds the most common chunks\n",
    "def np_chunk_counter(chunked_sentences):\n",
    "\n",
    "    # create a list to hold chunks\n",
    "    chunks = list()\n",
    "\n",
    "    # for-loop through each chunked sentence to extract noun phrase chunks\n",
    "    for chunked_sentence in chunked_sentences:\n",
    "        for subtree in chunked_sentence.subtrees(filter=lambda t: t.label() == 'NP'):\n",
    "            chunks.append(tuple(subtree))\n",
    "\n",
    "    # create a Counter object\n",
    "    chunk_counter = Counter()\n",
    "\n",
    "    # for-loop through the list of chunks\n",
    "    for chunk in chunks:\n",
    "        # increase counter of specific chunk by 1\n",
    "        chunk_counter[chunk] += 1\n",
    "\n",
    "    # return 30 most frequent chunks\n",
    "    return chunk_counter.most_common(30)\n",
    "\n",
    "\n",
    "# function that pulls chunks out of chunked sentence and finds the most common chunks\n",
    "def vp_chunk_counter(chunked_sentences):\n",
    "\n",
    "    # create a list to hold chunks\n",
    "    chunks = list()\n",
    "\n",
    "    # for-loop through each chunked sentence to extract verb phrase chunks\n",
    "    for chunked_sentence in chunked_sentences:\n",
    "        for subtree in chunked_sentence.subtrees(filter=lambda t: t.label() == 'VP'):\n",
    "            chunks.append(tuple(subtree))\n",
    "\n",
    "    # create a Counter object\n",
    "    chunk_counter = Counter()\n",
    "\n",
    "    # for-loop through the list of chunks\n",
    "    for chunk in chunks:\n",
    "        # increase counter of specific chunk by 1\n",
    "        chunk_counter[chunk] += 1\n",
    "\n",
    "    # return 30 most frequent chunks\n",
    "    return chunk_counter.most_common(30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "7d3e7bd2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/nicknut/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "dc47040c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence and word tokenize text here\n",
    "word_tokenized_text = word_sentence_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e5ce7fc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['``', 'good', 'morning', ',', 'pooh', 'bear', ',', \"''\", 'said', 'eeyore', 'gloomily', '.']\n"
     ]
    }
   ],
   "source": [
    "# store and print any word tokenized sentence here\n",
    "single_word_tokenized_sentence = word_tokenized_text[707]\n",
    "\n",
    "print(single_word_tokenized_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191d1eb1",
   "metadata": {},
   "source": [
    "## Part-of-speech Tag Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "0603deee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list to hold part-of-speech tagged sentences here\n",
    "pos_tagged_text = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b39a4538",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a for loop through each word tokenized sentence here\n",
    "\n",
    "  # part-of-speech tag each sentence and append to list of pos-tagged sentences here\n",
    "for tokenized_sentence in word_tokenized_text:\n",
    "    pos_tagged_text.append(pos_tag(tokenized_sentence))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "8af95d87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('``', '``'), ('good', 'JJ'), ('morning', 'NN'), (',', ','), ('pooh', 'NN'), ('bear', 'NN'), (',', ','), (\"''\", \"''\"), ('said', 'VBD'), ('eeyore', 'RBR'), ('gloomily', 'RB'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "# store and print any part-of-speech tagged sentence here\n",
    "single_pos_sentence = pos_tagged_text[707]\n",
    "\n",
    "print(single_pos_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be552549",
   "metadata": {},
   "source": [
    "## Chunk Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c4333ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define noun phrase chunk grammar\n",
    "np_chunk_grammar = \"NP: {<DT>?<JJ>*<NN.*>}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "c985dcde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create noun phrase RegexpParser\n",
    "np_chunk_parser = RegexpParser(np_chunk_grammar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "f807a549",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define verb phrase chunk grammar\n",
    "vp_chunk_grammar = \"VP: {<DT>?<JJ>*<NN><VB.*><RB.?>?}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "8ee7f3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create verb phrase RegexpParser object\n",
    "vp_chunk_parser = RegexpParser(vp_chunk_grammar)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "047c8383",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create lists to hold NP and VP chunked sentences\n",
    "np_chunked_text = []\n",
    "vp_chunked_text = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "162d0e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a for loop through each pos-tagged sentence\n",
    "for pos_sentence in pos_tagged_text:\n",
    "  # NP and VP chunk each sentence and append to respective list\n",
    "    np_chunked_text.append(np_chunk_parser.parse(pos_sentence))\n",
    "    vp_chunked_text.append(vp_chunk_parser.parse(pos_sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ade8062",
   "metadata": {},
   "source": [
    "## Analyze Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "17916ab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[((('pooh', 'NN'),), 293), ((('i', 'NN'),), 230), ((('piglet', 'NN'),), 179), ((('robin', 'NN'),), 144), ((('rabbit', 'NN'),), 101), ((('christopher', 'NN'),), 65), ((('owl', 'NN'),), 64), ((('kanga', 'NN'),), 60), ((('roo', 'NN'),), 57), ((('_', 'NN'),), 53), ((('something', 'NN'),), 46), ((('eeyore', 'NN'),), 42), ((('honey', 'NN'),), 37), ((('head', 'NN'),), 30), ((('i', 'NNS'),), 28), ((('anything', 'NN'),), 28), ((('home', 'NN'),), 27), ((('winnie-the-pooh', 'NN'),), 24), ((('nothing', 'NN'),), 23), ((('bear', 'NN'),), 22), ((('*', 'NNP'),), 21), ((('house', 'NN'),), 21), ((('the', 'DT'), ('water', 'NN')), 21), ((('course', 'NN'),), 20), ((('the', 'DT'), ('forest', 'NN')), 19), ((('oh', 'NN'),), 19), ((('hallo', 'NN'),), 18), ((('round', 'NN'),), 16), ((('dear', 'NN'),), 16), ((('case', 'NN'),), 15)]\n"
     ]
    }
   ],
   "source": [
    "# store and print the most common NP-chunks here\n",
    "most_common_np_chunks = np_chunk_counter(np_chunked_text)\n",
    "print(most_common_np_chunks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "9cfdc4ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[((('i', 'NN'), ('do', 'VBP'), (\"n't\", 'RB')), 16), ((('i', 'NN'), ('was', 'VBD')), 15), ((('i', 'NN'), ('said', 'VBD')), 10), ((('pooh', 'NN'), ('was', 'VBD')), 8), ((('i', 'NN'), (\"'m\", 'VBP')), 8), ((('piglet', 'NN'), ('said', 'VBD')), 8), ((('i', 'NN'), ('am', 'VBP')), 8), ((('i', 'NN'), ('suppose', 'VBP')), 7), ((('i', 'NN'), ('know', 'VBP')), 7), ((('i', 'NN'), ('think', 'VBP')), 7), ((('i', 'NN'), ('did', 'VBD'), (\"n't\", 'RB')), 7), ((('i', 'NN'), ('thought', 'VBD')), 7), ((('piglet', 'NN'), ('was', 'VBD')), 7), ((('robin', 'NN'), ('had', 'VBD')), 7), ((('i', 'NN'), ('did', 'VBD')), 7), ((('pooh', 'NN'), ('looked', 'VBD')), 6), ((('pooh', 'NN'), ('said', 'VBD')), 6), ((('robin', 'NN'), ('said', 'VBD')), 5), ((('owl', 'NN'), ('was', 'VBD')), 5), ((('i', 'NN'), ('had', 'VBD')), 5), ((('i', 'NN'), (\"'ve\", 'VBP')), 5), ((('i', 'NN'), ('have', 'VBP')), 4), ((('i', 'NN'), ('do', 'VBP')), 4), ((('rabbit', 'NN'), ('said', 'VBD')), 4), ((('robin', 'NN'), ('is', 'VBZ')), 4), ((('i', 'NN'), ('see', 'VBP')), 3), ((('a', 'DT'), ('heffalump', 'NN'), ('was', 'VBD')), 3), ((('pooh', 'NN'), ('rubbed', 'VBD')), 3), ((('pooh', 'NN'), ('did', 'VBD')), 3), ((('piglet', 'NN'), ('had', 'VBD')), 3)]\n"
     ]
    }
   ],
   "source": [
    "# store and print the most common VP-chunks here\n",
    "most_common_vp_chunks = vp_chunk_counter(vp_chunked_text)\n",
    "print(most_common_vp_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883a9005",
   "metadata": {},
   "source": [
    "## Observations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3692ea62",
   "metadata": {},
   "source": [
    "- Unsurprisingly, Pooh is the most commonly mentioned noun\n",
    "- Many of the other characters are also the top mentioned NPs. This gives us an idea who the main characters are.\n",
    "- Eeyore confused the POS tagger - it somehow is tagged as an adverb, which we saw earlier.\n",
    "- `I` is very common, too - much of the book must use first person, probaably in dialog\n",
    "- The top VPs are `I don't` and `I was` and `I said` ... which fits with the above observation\n",
    "- The rest of the VPs tend to describe what the characters are doing `piglet was`, `pooh said`, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feca5ec4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
